{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## In this work we removed old indexes (before 2007-04) because of very non understandable format specifications."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import traceback\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import ast\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_file = 'tools/arXiv_pdf_manifest_newindex.xml'\n",
    "meta_snap_file = 'tools/arxiv-metadata-oai-snapshot.json'\n",
    "meta_cat_file = 'tools/arxiv-metadata-ext-category.csv'\n",
    "meta_tax_file = 'tools/arxiv-metadata-ext-taxonomy.csv'\n",
    "\n",
    "in_categories = 'categories.txt'\n",
    "\n",
    "year_choice = '2020'\n",
    "month_choice = '09'\n",
    "seq_nums = range(1, 30)\n",
    "\n",
    "out_dir = 'data'\n",
    "mode = 'pdf'\n",
    "log_file_path = 'logs/log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'pdf' and mode != 'src':\n",
    "    raise Exception('mode should be \"pdf\" or \"src\"')"
   ]
  },
  {
   "source": [
    "# s3cmd\n",
    "First thing is to set s3cmd environment with your personal configuration keys. You will need to set up a AWS account, and set the payment card in order to pay for the arXiv download. In fact, the arXiv buckets are configurated as \"requester-pay\", so you'll be charged for each download. Not be intimidated: you'll pay around 0,02€ for each GB you download, but remember that the whole arXive is around 2TB that costs you a bit less than 50€.\n",
    "\n",
    "You need to set s3cmd with the following command:\n",
    "\n",
    "$ s3cmd --configure\n",
    "\n",
    "and enter ACCESS_KEY and SECRET_KEY. After this, you are right to go.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(fname, out_dir):\n",
    "    cmd = ['s3cmd', 'get', '--requester-pays',\n",
    "           's3://arxiv/%s' % fname, './%s' % out_dir]\n",
    "    print(' '.join(cmd))\n",
    "    # subprocess.call(' '.join(cmd), shell=True)    "
   ]
  },
  {
   "source": [
    "# Utils\n",
    "\n",
    "We need some utils function, so we create, first of all, the functions we need in order to manage the data and save the useful files for further uses."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_to_ordered_list(path_to_file):\n",
    "  \"\"\"\"\"\n",
    "  Arguments:\n",
    "    - path_to_file: path to file 'arxiv-metadata-ext-taxonomy.csv' composed by a list of all categories that forms the arXiv taxonomy.\n",
    "\n",
    "  Output:\n",
    "    - meta_ls: list of all categories and description, grouped by archive without intra group divisions (ex. just 'cs' and not 'cs.AI', etc)\n",
    "    - unordered: list of all categories in the order are proveded in 'arxiv-metadata-ext-taxonomy.csv' file.\n",
    "  \"\"\"\"\"\n",
    "\n",
    "  meta_ls = []\n",
    "  unordered = []\n",
    "  with open(path_to_file, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    header = next(csvreader)\n",
    "    for row in csvreader:\n",
    "      meta_ls.append([row[4].split('.')[0] if '.' in row[4] else row[4], row[1]])\n",
    "      unordered.append(row[4])\n",
    "  meta_ls = [list(item) for item in set(tuple(row) for row in meta_ls)]\n",
    "  unordered = list(set(unordered))\n",
    "  return sorted(meta_ls), unordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_snapshot_to_json(path_to_file, meta_tax_unordered):\n",
    "  \"\"\"\"\"\n",
    "  Arguments:\n",
    "    - path_to_file: path to file 'arxiv-metadata-oai-snapshot.json' composed by a list of all papers with id, title, authors, abstract and others properties.\n",
    "\n",
    "  Output:\n",
    "    - df_data_id: dictionary with \"id\" key and dictionary values composed by \"id, nid (modified id to represent old and new id in one single representation), and categories\"\n",
    "    - df_data_nid: dictionary with \"nid\" key and dictionary values composed by \"id, nid (modified id to represent old and new id in one single representation), and categories\"\n",
    "    - ls_data: list of ordered \"nid\" values in order to obtain a sub list in between two valued (necessary to comprehend which pdf are in a .tar file, and of which category).\n",
    "  \"\"\"\"\"\n",
    "  df_data_id = {}\n",
    "  df_data_nid = {}\n",
    "  ls_data = []\n",
    "  last = 0\n",
    "  with open(path_to_file) as json_file:\n",
    "    for j,line in enumerate(json_file):\n",
    "      single_dict = {}\n",
    "      # 1382795\n",
    "      # 1796911\n",
    "      if j > 1382793:\n",
    "        break\n",
    "\n",
    "      if int(j/1382795*100) != last:\n",
    "        last = int(j/1382795*100)\n",
    "        print(' ['+ str(last) +'%] '+str(j)+'/1382795')\n",
    "      row = json.loads(line)\n",
    "      single_dict['id'] = row['id']\n",
    "      single_dict['nid'] = '20'+row['id'] if '.' in row['id'] else ('20'+str(row['id'][-7:-3]) if str(row['id'][-7:-3]) < \"50\" else '19'+str(row['id'][-7:-3]))+'.'+get_number(meta_tax_unordered, row['id'])+str(row['id'][-3:])\n",
    "      single_dict['categories'] = row['categories']\n",
    "      df_data_id[single_dict['id']] = single_dict\n",
    "      df_data_nid[single_dict['nid']] = single_dict\n",
    "      ls_data.append(single_dict['nid'])\n",
    "  return df_data_id, df_data_nid, sorted(ls_data)"
   ]
  },
  {
   "source": [
    "Once that we have saved our useful files, we could start the script using the already created files instead of calsulate everithing from scratch, again. So we'll try to load the saved files, if something goes wrong, we will proceed creating those again, but if everithing goes right, we will proceed stright forward with the loaded files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict_from_csv(path_to_file):\n",
    "  meta_df = {}\n",
    "  with open(path_to_file, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    # header = next(csvreader)\n",
    "    for row in csvreader:\n",
    "      first, second = row[0], row[1]\n",
    "      \n",
    "      meta_df[first] = second\n",
    "      \n",
    "  return meta_df\n",
    "\n",
    "def read_list_from_txt(path_to_file):\n",
    "  meta_ls = []\n",
    "  with open(path_to_file, 'r') as txtfile:\n",
    "    for row in txtfile:\n",
    "      meta_ls.append(row.strip())\n",
    "  return meta_ls"
   ]
  },
  {
   "source": [
    "Then we need the funtions to save our dictionaries and lists to file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_csv(file_dict, filt_to_save):\n",
    "  w = csv.writer(open(filt_to_save, \"w\"))\n",
    "  for key, val in file_dict.items():\n",
    "        w.writerow([key, val])\n",
    "\n",
    "def save_list_to_txt(file_list, filt_to_save, to_set=True, sort=True):\n",
    "  unique_list = file_list\n",
    "  if to_set: \n",
    "    unique_list = set(unique_list)\n",
    "  if sort:\n",
    "    unique_list = sorted(unique_list)\n",
    "  with open(filt_to_save, 'w') as f:\n",
    "    for item in unique_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(log_file_path, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Check if files exist\n",
      "Files exist, load them correctly\n"
     ]
    }
   ],
   "source": [
    "  try:\n",
    "    print(\"Check if files exist\")\n",
    "    meta_snap_dict_id = read_dict_from_csv('backups/metadata_snap_dict_id.csv')\n",
    "    meta_snap_dict_nid = read_dict_from_csv('backups/metadata_snap_dict_nid.csv')\n",
    "    meta_cat_list = read_list_from_txt('backups/metadata_cat_list_nid.txt')\n",
    "    meta_tax_list = read_list_from_txt('backups/selected_categories.txt')\n",
    "    print(\"Files exist, load them correctly\")\n",
    "    \n",
    "  except IOError:\n",
    "    print(\"Files don't exist\")\n",
    "    print(\"Re make those\")\n",
    "    meta_tax_list, meta_tax_unordered = categories_to_ordered_list(meta_tax_file)\n",
    "    meta_snap_dict_id, meta_snap_dict_nid, meta_cat_list = input_snapshot_to_json(meta_snap_file, meta_tax_unordered)\n",
    "\n",
    "    print(\"Save files for future\")\n",
    "    save_dict_to_csv(meta_snap_dict_id, 'backups/metadata_snap_dict_id.csv')\n",
    "    save_dict_to_csv(meta_snap_dict_nid, 'backups/metadata_snap_dict_nid.csv')\n",
    "    save_list_to_txt(meta_cat_list, 'backups/metadata_cat_list_nid.txt')\n",
    "    save_list_to_txt(meta_tax_list, 'backups/selected_categories.txt', False, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def input_categories_to_list(path_to_file):\n",
    "  \"\"\"\"\"\n",
    "  Arguments:\n",
    "    - path_to_file: path to file 'categories.txt' composed by a list of all categories we are interested in. It can contain commented lines (\"#\") and empty lines (\"\\n\") that will be avoided.\n",
    "\n",
    "  Output:\n",
    "    - in_ls: list of all categories we are interested in.\n",
    "  \"\"\"\"\"\n",
    "  in_ls = []\n",
    "  with open(path_to_file, 'r') as txtfile:\n",
    "    for row in txtfile:\n",
    "      line = txtfile.readline()\n",
    "      if line[0] != '#' and line != '\\n':\n",
    "        in_ls.append(line.strip())\n",
    "\n",
    "  return sorted(in_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict_list_from_csv(path_to_file):\n",
    "  \"\"\"\"\"\n",
    "  Arguments:\n",
    "    - path_to_file: path to file 'arxiv-metadata-ext-category.csv' composed by a list of papers and its categories.\n",
    "\n",
    "  Output:\n",
    "    - meta_df: list of all categories for each paper.\n",
    "  \"\"\"\"\"\n",
    "  meta_df = {}\n",
    "  with open(path_to_file, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    # header = next(csvreader)\n",
    "    for row in csvreader:\n",
    "      first, second = row[0], row[1]\n",
    "      if first in meta_df:\n",
    "          meta_df[first].append(second)\n",
    "      else:\n",
    "          meta_df[first] = [second] \n",
    "      \n",
    "  return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_cat_list = input_categories_to_list(in_categories)\n",
    "meta_tax_list = read_dict_list_from_csv('tools/arxiv-metadata-ext-category.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tar_categories_list(fitem, litem, meta_snap_dict_id, meta_snap_dict_nid, meta_cat_list):\n",
    "  \"\"\"\"\"\n",
    "  Arguments:\n",
    "    - fitem: first item id in tar file\n",
    "    - litem: last item id in tar file\n",
    "    - meta_snap_dict_id: dictionary with keys \"id\" and values \"dictionary with id, nid, and categories\" for each paper\n",
    "    - meta_snap_dict_nid: dictionary with keys \"nid\" and values \"dictionary with id, nid, and categories\" for each paper\n",
    "    - meta_cat_list: list loaded from 'metadata_cat_list_nid.txt' composed by all nid index ordered by date\n",
    "\n",
    "  Output:\n",
    "    - step_seven [None]: list of all categories classes that occur at least once in the arxiv tar papers. It is None if first index is bigger than last index. It used to occure whith old format index so we removed old indexes (before 2007-04)\n",
    "  \"\"\"\"\"\n",
    "  fid = fitem if '.' in fitem else str(fitem[:-7])+'/'+str(fitem[-7:])\n",
    "  lid = litem if '.' in litem else str(litem[:-7])+'/'+str(litem[-7:])\n",
    "  \n",
    "  fnid = ast.literal_eval(meta_snap_dict_id[fid])\n",
    "  lnid = ast.literal_eval(meta_snap_dict_id[lid])\n",
    "\n",
    "  findex = meta_cat_list.index(fnid['nid'])\n",
    "  lindex = meta_cat_list.index(lnid['nid'])\n",
    "\n",
    "  step_one = [meta_snap_dict_nid[meta_cat] for meta_cat in meta_cat_list[findex:lindex+1]]\n",
    "  \n",
    "  step_two = [ast.literal_eval(meta_snap)['categories'] for meta_snap in step_one]\n",
    "  \n",
    "  step_three = [ele.split(' ') for ele in step_two]\n",
    "  \n",
    "  step_four = [[el.split('.')[0] if '.' in el else el for el in ele ] for ele in step_three]\n",
    "  \n",
    "  step_five = reduce(lambda x,y: x+y, step_four)\n",
    "  \n",
    "  step_six = set(step_five)\n",
    "\n",
    "  step_seven = list(step_six)\n",
    "\n",
    "  return step_seven if findex < lindex else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "  lst3 = [value for value in lst1 if value in lst2] \n",
    "  return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_to_download(cat_list, in_cat_list):\n",
    "  inter = intersection(cat_list, in_cat_list)\n",
    "  return False if not inter else True, inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5260372646152973 GB\n",
      "Finished\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-24-491a2e005694>\", line 32, in <module>\n",
      "    choice, intersection = paper_to_download(cat_list, in_cat_list)\n",
      "  File \"<ipython-input-23-9100a4ea0e3d>\", line 2, in paper_to_download\n",
      "    inter = intersection(cat_list, in_cat_list)\n",
      "TypeError: 'list' object is not callable\n"
     ]
    }
   ],
   "source": [
    "total_size = 0\n",
    "\n",
    "try:\n",
    "  for event, elem in ET.iterparse(manifest_file):\n",
    "    \n",
    "    tag = elem.tag\n",
    "    value = elem.text\n",
    "\n",
    "    if event == 'end':\n",
    "          \n",
    "      if tag != 'file':\n",
    "        if value == None:\n",
    "          raise Exception('None occured in start when tag != file at: ', fname, fitem, litem, tag)\n",
    "\n",
    "        elif tag == 'filename' :\n",
    "          fname = value \n",
    "        elif tag == 'first_item' :\n",
    "          fitem = value \n",
    "        elif tag == 'last_item' :\n",
    "          litem = value \n",
    "        elif tag == 'size' :\n",
    "          size = int(value) \n",
    "\n",
    "      elif tag == 'file':\n",
    "\n",
    "        cat_list = tar_categories_list(fitem, litem, meta_snap_dict_id, meta_snap_dict_nid, meta_cat_list)\n",
    "        \n",
    "        if cat_list == None:\n",
    "          raise Exception('None occured in categories list: first index > last index. ** If you are working also with old index formatted papers (before 2007-04) have a look to the Introduction at the beginnig. **')\n",
    "\n",
    "        \n",
    "        choice, intersection = paper_to_download(cat_list, in_cat_list)\n",
    "\n",
    "        total_size += size\n",
    "\n",
    "        # get_file(fname, out_dir='%s/%s/' % (out_dir, mode))\n",
    "        \n",
    "        log_file.write(str(fname) + '\\t' + str(choice) + '\\t' + str(intersection) + '\\t' + str(size / 1073741824)+' GB' + '\\n')\n",
    "\n",
    "        print(str(total_size / 1073741824)+' GB')\n",
    "    \n",
    "    elem.clear()\n",
    "except:\n",
    "  traceback.print_exc()\n",
    "\n",
    "print('Finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}